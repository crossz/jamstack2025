# 《Kubernetes设计模式》深度解析与实战课程 (PBL问题导向重构版)

---

### **第一周: 课堂讲授 (理论2学时)**
> 第一章 引言与云原生基础

* #### **本周议题: 从单体到云原生，我们遇到了什么问题？**
    * [cite_start]**核心问题:** “我有一个庞大的单体应用，每次更新都像一场灾难，牵一发而动全身，而且难以针对性地扩展某个功能。我该如何改造我的软件架构，使其在云时代更具弹性、更易维护？” [cite: 232, 233]
    * **解决方案: 架构的演进**
        * [cite_start]引入**微服务架构**作为解决单体应用困境的思路，将复杂系统分解为一组小而专注的服务。 [cite: 232, 234]
        * [cite_start]讲解**云原生之路 (The Path to Cloud Native)** [cite: 243, 244]，阐述这不仅是技术的变革，更是思想、文化和流程的全面升级。
* #### **《实战》结合**：云原生基础
- Kubernetes架构图解（Master/Node组件）→ 《项目1-部署Kubernetes集群.pptx》P27 
- containerd与Kubernetes关系 → 《项目1》P5（知识点1-2） 
- 云原生核心价值（弹性、可观测性、韧性）→ 《项目1》P5-P8
- 微服务架构演进路径（单体→SOA→微服务）→ 《项目9-部署项目到Kubernetes集群.pptx》P4

---

### **第二周: 课堂讲授与实验 (理论1学时, 实验1学时)**
> 第一章 引言与云原生基础

* #### **本周议题 (理论): 如何标准化地打包和运行我的微服务？**
    * **核心问题:** “我的微服务（Java, Python, Go...）在我的开发机上能跑，但到了测试或生产环境就因为依赖、库版本不同而失败。如何才能实现‘一次构建，到处运行’？”
    * **解决方案: 容器化**
        * [cite_start]引入**容器(Container)**作为现代应用的标准交付单元，它将代码和所有依赖项打包在一起。 [cite: 279, 287]
        * [cite_start]容器是不可变的，一旦构建，其内部环境便固定下来，确保了环境的一致性。 [cite: 291]
    * #### **动手实践 (实验)**
        * **目标:** 亲手将一个简单的应用打包成容器镜像。
        * 安装Docker环境。
        * 编写一个简单的Dockerfile。
        * 使用`docker build`构建镜像，并用`docker run`运行容器。
* #### **《实战》结合**：容器化实践
containerd安装步骤 → 《项目1》P12（技能目标8） 
容器运维命令（crictl）→ 《项目1》P9
容器镜像分层原理 → 《项目5-配置数据存储.pptx》P8
Dockerfile多阶段构建 → 《项目10-构建企业级DevOps云平台.pptx》P15

---

### **第三周: 课堂讲授 (理论2学时)**
> 第一章 第二章

* #### **本周议题: 如何管理成百上千的容器？**
    * **核心问题一:** “现在我有了几十个微服务的容器镜像，我该如何部署它们、在它们故障时自动重启、将它们连接成一个网络、并根据负载进行伸缩？手动管理是不可能的。”
        * **解决方案: 容器编排**
            * [cite_start]引入**Kubernetes**作为容器编排的事实标准。 [cite: 83, 89]
            * 讲解Kubernetes的核心抽象如何解决这些管理难题：
                * [cite_start]**Pod:** 部署和运行容器的原子单元。 [cite: 304]
                * [cite_start]**Service:** 为一组动态变化的Pod提供一个稳定的访问入口和负载均衡。 [cite: 329, 333]
                * [cite_start]**标签(Labels):** 用于组织和选择资源对象，是Service与Pod关联的纽带。 [cite: 337]
                * [cite_start]**命名空间(Namespaces):** 用于在集群内划分逻辑隔离区。 [cite: 370, 372, 373]

    * **核心问题二:** “在共享的集群环境中，如果不对资源加以规划，可能会导致资源浪费或关键应用因资源不足而失败。我们该如何开始思考资源规划？”
        * **解决方案: 容量规划初步**
            * [cite_start]引入**可预测的需求(Predictable Demands)**的概念：应用必须声明其资源需求，这是成功部署和共存的基础。 [cite: 407, 409]
            * [cite_start]讲解**容量规划(Capacity Planning)**的重要性，即根据所有服务的需求来规划整个集群的物理资源。 [cite: 419, 553]
* #### **《实战》结合**：Kubernetes核心资源
Pod共享网络/存储特性 → 《项目2-使用集群核心资源部署服务.pptx》P6 
Service类型对比（ClusterIP/NodePort）→ 《项目6-使用Ingress发布服务.pptx》P14 
Pod生命周期（Init容器）→ 《项目4-调度Pod到指定节点.pptx》P18 
标签选择器实战 → 《项目2》P11（任务2.1.3）

---

### **第四周: 课堂讲授与实验 (理论1学时, 实验1学时)**
> 第二章 可预测的需求

* #### **本周议题 (理论): 无约束的容器会带来什么灾难？**
    * **核心问题:** “如果我直接在服务器上运行一个容器，不加任何资源限制，最坏会发生什么情况？这与在Kubernetes中运行有何不同？”
    * **解决方案: 理解资源的本质**
        * [cite_start]区分**可压缩资源(CPU)**和**不可压缩资源(Memory)**。 [cite: 452, 453]
        * [cite_start]当内存耗尽时，应用进程会被操作系统无情地**杀死(OOMKilled)**。 [cite: 454]
        * [cite_start]当CPU繁忙时，应用只会被**节流(throttled)**，性能下降但不会崩溃。 [cite: 454]
        * 对比`docker run`的资源限制参数与Kubernetes的声明式资源管理。
    * #### **动手实践 (实验)**
        * **目标:** 亲眼见证资源不受控的后果。
        * 使用`docker run`启动一个无内存限制的程序，并使其不断申请内存，观察容器被杀死的现象。
        * 对比`kubectl`和`docker`的基础命令，感受编排工具与容器运行时的区别。
* #### **《实战》结合**：资源管理实战
资源超限后果（OOMKilled）→ 《项目4》P10 
HPA自动扩缩容策略 → 《项目8-使用Operator自定义控制器部署应用.pptx》P22 
cgroup原理 → 《项目5》P7（知识点2）
资源配额监控 → 《项目11-使用Python管理Kubernetes集群.pptx》P13

---

### **第五周: 课堂讲授 (理论2学时)**
> 第二章 第三章

* #### **本周议题: 如何在K8s中精细化地管理和分配资源？**
    * **核心问题一:** “我如何向Kubernetes精确地描述我的应用：‘平时需要0.5个CPU，忙时最多不能超过2个CPU；至少需要1GB内存，但绝不能超过2GB’？K8s如何利用这些信息？”
        * **解决方案: K8s资源管理核心**
            * [cite_start]深入讲解**资源请求(Requests)**和**资源限制(Limits)**。 [cite: 455, 456] `Requests`用于调度，`Limits`用于运行时强制约束。
            * [cite_start]讲解**Pod优先级(Pod Priority)**，允许为关键应用赋予更高的调度和保留优先级。 [cite: 494]

    * **核心问题二:** “作为集群管理员，如何防止某个项目组‘挥霍无度’，用光整个集群的资源？如何为不同环境（如开发、生产）设定资源使用上限？”
        * **解决方案: 命名空间级的资源治理**
            * [cite_start]**ResourceQuota:** 限制一个命名空间内可创建的资源对象数量以及资源总量（CPU、内存）。 [cite: 529, 531]
            * [cite_start]**LimitRange:** 为命名空间内的Pod设置默认的、最小、最大的资源请求/限制值。 [cite: 536]

    * **核心问题三 (过渡):** “现在我的Pod资源可控了，下一步是如何管理它们的更新和发布？”
        * **解决方案: 声明式部署**
            * [cite_start]引入`Declarative Deployment`模式的概念，即通过一个YAML文件“声明”应用的最终状态，由K8s负责达成。 [cite: 577]
* #### **《实战》结合**：声明式部署
Deployment滚动更新流程 → 《项目6》P21
ResourceQuota配置示例 → 《项目3-认证授权用户访问集群资源.pptx》P8 
GitOps实践（Argo CD）→ 《项目10》P18
配置漂移防护 → 《项目7-使用Helm包管理工具部署应用.pptx》P12

---

### **第六周: 课堂讲授与实验 (理论1学时, 实验1学时)**
> 第三章 声明式部署

* #### **本周议题 (理论): 如何实现应用的自动化、零停机更新？**
    * **核心问题:** “我发布了应用的新版本v1.1，如何让K8s自动地、平滑地用新版本Pod替换掉所有v1.0的Pod，期间不能中断用户服务？如果新版本有问题，如何快速撤销操作？”
    * **解决方案: Deployment与滚动更新**
        * [cite_start]讲解`Deployment`资源如何通过**滚动更新(RollingUpdate)**策略来实现零停机更新。 [cite: 588, 591, 607, 618]
        * [cite_start]介绍`kubectl rollout undo`命令，作为一键回滚的“后悔药”。 [cite: 601]
    * #### **动手实践 (实验)**
        * **目标:** 亲手操作一次完整的应用发布与回滚流程。
        * 创建一个`Deployment`资源。
        * 修改`Deployment`中的镜像标签，触发一次滚动更新。
        * 使用`kubectl rollout status`观察更新进度。
        * 使用`kubectl rollout undo`将应用恢复到上一个版本。
* #### **《实战》结合**：发布策略
Ingress灰度发布配置 → 《项目6》P24 
Helm部署MySQL案例 → 《项目7》P7 
金丝雀发布流量切分 → 《项目6》P16（图6-3） 
回滚的etcd数据版本管理 → 《项目5》P15

---

### **第七周: 课堂讲授 (理论2学时)**
> 第三章 第四章

* #### **本周议题: 除了滚动更新，还有哪些更高级、更安全的发布策略？**
    * **核心问题一:** “对于一些有破坏性变更或非常关键的服务，滚动更新（新旧版本共存）可能会带来问题。我能否先部署好新版本，测试无误后，瞬间将所有流量切换过去？（蓝绿部署）”
        * **解决方案: 蓝绿部署 (Blue-Green Release)**
            * [cite_start]讲解其原理：同时保留新旧两套环境，通过修改`Service`的`selector`来实现流量的瞬时切换。 [cite: 649, 650, 657]

    * **核心问题二:** “我对新版本信心不足，不敢直接全量。我能否先让一小部分用户（比如5%）使用新版本，观察其表现稳定后再逐步扩大范围？（金丝雀发布）”
        * **解决方案: 金丝雀发布 (Canary Release)**
            * [cite_start]讲解其原理：通过维护两个不同版本的`Deployment`，并调整副本数或使用更高级的流量管理工具来实现按比例的流量切分。 [cite: 662, 663]

    * **核心问题三 (过渡):** “所有这些自动化的发布策略，都依赖一个前提：系统必须知道新启动的Pod是否真的‘能工作’。这如何实现？”
        * **解决方案: 健康度探针**
            * [cite_start]引入`Health Probe`模式，特别是**就绪探针(Readiness Probes)**，它是决定一个Pod能否接收流量的关键。 [cite: 709, 711]
* #### **《实战》结合**：健康探针
就绪探针（Readiness）作用 → 《项目6》P25 
启动探针（Startup）配置 → 《项目8》P28
探针超时设置 → 《项目4》P20
故障注入测试方法 → 《项目9》P12


---

### **第八周: 课堂讲授与实验 (理论1学时, 实验1学时)**
> 第四章 健康度指针

* #### **本周议题 (理论): 如何让K8s真正了解我的应用健康状况？**
    * **核心问题:** “我的应用进程还在，但内部可能已经因为死锁或依赖服务故障而无法正常工作。如何让K8s不仅检查‘进程存活’，更能检查‘业务健康’，并在出问题时实现故障自愈？”
    * **解决方案: 存活探针与就绪探针的协同**
        * [cite_start]**就绪探针(Readiness Probe):** 决定是否将Pod加入服务负载均衡。失败则摘流。 [cite: 753, 754]
        * [cite_start]**存活探针(Liveness Probe):** 决定Pod是否需要被重启。失败则重启。 [cite: 731]
    * #### **动手实践 (实验)**
        * **目标:** 综合运用健康探针，实现平滑升级与故障自愈。
        * 为一个`Deployment`配置好`readinessProbe`和`livenessProbe`。
        * 执行一次滚动更新，观察`readinessProbe`如何确保新Pod准备好后才替换旧Pod。
        * 手动模拟应用故障（如`touch /tmp/unhealthy`），让`livenessProbe`失败，观察K8s自动重启该Pod。
* #### **《实战》结合**

---

### **第九周: 课堂讲授 (理论2学时)**
> 第四章 第五章

* #### **本周议题: 应用的“生”与“死”，我们该如何干预？**
    * **核心问题一:** “我还是对存活探针和就绪探针感到困惑。它们都检查应用的健康，到底该在什么场景下分别使用它们？”
        * **解决方案: 深入辨析探针**
            * **就绪探针**回答的是：“你现在能营业吗？” (Is it ready for traffic?)。用于应用启动、依赖加载、临时过载等场景。
            * **存活探针**回答的是：“你还活着吗？” (Is it broken?)。用于检测死锁等无法自行恢复的致命错误。
            * [cite_start]引入**启动探针(Startup Probe)**解决启动时间过长的应用的探针配置难题。 [cite: 777, 780]

    * **核心问题二:** “当K8s决定要终止我的Pod时，我希望应用能有机会完成当前正在处理的请求，优雅地关闭数据库连接，而不是被粗暴地‘杀死’。我该如何实现这种‘优雅停机’？”
        * **解决方案: 生命周期管理**
            * [cite_start]引入**`Managed Lifecycle`**模式。 [cite: 817]
            * [cite_start]讲解`SIGTERM`信号，这是K8s发出的“请准备关闭”的通知。 [cite: 836, 837]
            * [cite_start]讲解**生命周期钩子(Lifecycle Hooks)**，特别是`preStop`钩子，它是在收到`SIGTERM`信号后、容器被杀死前执行的关键回调，是实现优雅停机的最佳实践。 [cite: 863, 864, 869]
* #### **《实战》结合**：生命周期管理
PreStop钩子优雅停机 → 《项目5》P20 
SIGTERM信号处理 → 《项目4》P18
PodDisruptionBudget（PDB）→ 《项目8》P24
数据库连接释放实践 → 《项目9》P18

---

### **第十周: 课堂讲授与实验 (理论1学时, 实验1学时)**
> 第五章 生命周期管理

* #### **本周议题 (理论): 如何在实践中实现优雅停机？**
    * **核心问题:** “理论上我知道了`preStop`钩子，那么在我的Pod YAML文件中，具体应该怎么写？它可以执行哪些类型的操作？”
    * **解决方案: `preStop`钩子实践**
        * [cite_start]展示`preStop`钩子的两种实现方式：执行一个shell命令 (`exec`) 或发送一个HTTP GET请求 (`httpGet`)。 [cite: 859]
        * [cite_start]强调`preStop`是一个阻塞操作，K8s会等待它执行完成（或超时）再杀死容器。 [cite: 863]
    * #### **动手实践 (实验)**
        * **目标:** 观察并验证优雅停机过程。
        * 部署一个带有`preStop`钩子的Pod，钩子内容为`sh -c 'echo "Preparing to shutdown..."; sleep 30'`。
        * 执行`kubectl delete pod`，并立即用`kubectl describe pod`观察Pod状态，会看到它长时间处于`Terminating`状态，并能看到相关的事件，验证了钩子的执行。
        * （选做）搭建一个简单的K8s集群（如使用kind或kubeadm）。
* #### **《实战》结合**：集群搭建
kubeadm高可用集群 → 《项目1》P20
证书轮换（cert-manager）→ 《项目3》P15
集群网络方案对比 → 《项目1》P25
灾备恢复策略 → 《项目5》P22

---

### **第十一周: 课堂讲授 (理论2学时)**
> 第五章 第六章

* #### **本周议题: 如何让K8s智能地为我的Pod“安家”？**
    * **核心问题一:** “我的K8s集群里有各种各样的节点：有的CPU强，有的内存大，有的带GPU。我如何能‘告诉’K8s我的Pod的偏好，让它被调度到最合适的节点上？”
        * **解决方案: 容器的自动调度 (Automated Placement)**
            * [cite_start]讲解K8s**调度器(Scheduler)**的基本工作原理：过滤(Filtering)和打分(Scoring)。 [cite: 934, 975]
            * [cite_start]引入**节点亲和性(Node Affinity)**，允许Pod表达对节点特性的“喜爱”程度（必须满足或偏好满足）。 [cite: 992, 993]

    * **核心问题二:** “除了节点特性，Pod之间的关系也很重要。比如，为了高可用，服务的多个副本必须分开部署；为了低延迟，应用和它依赖的缓存最好部署在一起。这又该如何实现？”
        * **解决方案: Pod间的关系调度**
            * [cite_start]**Pod亲和性与反亲和性(Pod Affinity/Anti-Affinity):** 定义Pod与已存在Pod之间的部署关系。 [cite: 1005]
            * [cite_start]**污点与容忍度(Taints and Tolerations):** 允许节点“拒绝”不匹配的Pod，实现专用节点等场景。 [cite: 1039, 1041]
* #### **《实战》结合**：调度策略
节点亲和性配置 → 《项目4》P15 
污点容忍度用例 → 《项目4》P12
拓扑分布约束 → 《项目4》P18
GPU节点隔离策略 → 《项目4》P22

调度器过滤/打分机制 → 《项目4》P10 
多维度调度优化 → 《项目4》P22
Descheduler负载均衡 → 《项目4》P24
成本优化调度 → 《项目11》P18

---

### **第十二周: 课堂讲授与实验 (理论1学时, 实验1学时)**
> 第六章 容器的调度策略

* #### **本周议题 (理论): 如何组合使用调度规则解决复杂场景？**
    * **核心问题:** “在真实的生产环境中，一个应用可能同时有多种调度需求：它需要运行在有SSD的节点上，同时它的副本之间还要相互反亲和。如何将这些规则组合起来？”
    * **解决方案: 调度策略的叠加**
        * 通过一个综合案例，展示如何在同一个Pod的YAML中同时定义`nodeAffinity`和`podAntiAffinity`。
        * 讲解不同规则之间的逻辑关系（通常是“与”关系）。
    * #### **动手实践 (实验)**
        * **目标:** 综合运用本单元所学的调度知识。
        * 设计一个场景：部署一个3副本的应用，要求它必须运行在`region: us-east-1`的节点上，并且3个副本必须分布在不同的`availability-zone`中。
        * 学生需要为节点打上相应的标签，并编写包含`nodeAffinity`和`podAntiAffinity`的`Deployment` YAML文件来实现这一目标。
* #### **《实战》结合**

---

### **第十三周: 课堂讲授 (理论2学时)**
> 第六章 第七章

* #### **本周议题: 如何在K8s中处理“一次性”和“定时”任务？**
    * **核心问题:** “我的应用场景中，除了需要7x24小时运行的在线服务，还有两种常见任务：一种是需要执行一次就结束的数据迁移脚本；另一种是需要每天凌晨定时运行的报表生成程序。如何用K8s来管理这类非长时运行的任务？”
    * **解决方案: 引入第七章的行为模式 - 任务型作业**
        * [cite_start]**`Batch Job` (批量作业):** 针对“一次性”任务，使用`Job`资源。它会创建Pod来执行任务，并确保任务成功完成（如果失败会重试），完成后Pod不会被清理（便于检查日志），`Job`对象状态变为`Completed`。 [cite: 1138, 1139, 1152, 1153]
        * [cite_start]**`Periodic Job` (周期性作业):** 针对“定时”任务，使用`CronJob`资源。你只需提供一个标准的`cron`表达式和`Job`模板，`CronJob`控制器就会像一个高可用的分布式crontab一样，周期性地为你创建`Job`来执行任务。 [cite: 1238, 1253, 1256, 1257]
        * **对比:** `Job`是执行体，`CronJob`是调度器。
* #### **《实战》结合**：任务作业
CronJob表达式语法 → 《项目2》P25
Job失败重试策略 → 《项目11》P13
并行任务控制（Indexed）→ 《项目2》P27
历史记录清理 → 《项目7》P15

---

### **第十四周: 课堂讲授与实验 (理论1学时, 实验1学时)**
> 第七章 行为模式

* #### **本周议题 (理论): 如何管理那些需要在每个节点上运行的“特殊”服务？**
    * **核心问题一:** “我需要部署一个日志收集Agent（如Fluentd）或是一个节点监控程序（如Node Exporter）。这类基础服务有一个共同特点：它们必须在集群中的**每一个**工作节点上都运行一个实例。我该如何实现这种部署模式？”
        * **解决方案: `Daemon Service` (守护服务)**
            * [cite_start]使用`DaemonSet`资源。它会忽略调度器，直接在所有（或匹配`nodeSelector`的）节点上创建一个Pod副本，并确保每个节点上始终有且仅有一个。 [cite: 1284, 1285, 1300]

    * **核心问题二:** “我需要部署一个高可用的单例服务，比如一个集群中的主节点。它必须是唯一的，但又不能有单点故障。`StatefulSet`似乎可以，它和`DaemonSet`有什么区别？”
        * **解决方案: `Singleton Service` (单例服务) 与对比**
            * `DaemonSet`的目标是“每个节点一个”，总副本数等于节点数。
            * [cite_start]`StatefulSet`（当`replicas=1`时）的目标是“整个集群仅一个”，实现强一致性的单例。 [cite: 1352, 1376] `StatefulSet`更关注单个实例的唯一性和数据持久性。
    * #### **动手实践 (实验)**
        * **目标:** 部署一个`DaemonSet`和一个单例`StatefulSet`。
        * 部署一个简单的`DaemonSet`（如busybox），并验证每个工作节点上都有一个对应的Pod。
        * 部署一个`replicas: 1`的`StatefulSet`，观察其稳定的Pod名称和PVC。
* #### **《实战》结合**：守护服务
DaemonSet滚动更新 → 《项目8》P20
节点污点协同策略 → 《项目8》P14
Redis集群槽位分配 → 《项目8》P26
基础设施节点管理 → 《项目8》P18

---

### **第十五周: 课堂讲授 (理论2学时)**
> 第七章 行为模式

* #### **本周议题: “无状态” vs “有状态”，我该如何选择和部署？**
    * **核心问题:** “在K8s中，最核心的一对概念就是无状态应用和有状态应用。我什么时候应该用`Deployment`，什么时候又必须用`StatefulSet`？它们在存储和网络方面到底有什么根本区别？”
    * **解决方案: 深入对比两大核心部署模式**
        * [cite_start]**`Stateless Service` (无状态服务)[cite: 1483, 1491]:**
            * **适用场景:** Web服务器、API网关等。
            * **控制器:** `Deployment`。
            * **特点:** 所有Pod完全相同，可任意替换和伸缩（“牛群”）。
            * [cite_start]**网络:** 通过一个普通的`ClusterIP` Service提供统一入口和负载均衡。 [cite: 1529]
            * [cite_start]**存储:** 通常不直接管理持久化存储，或所有副本共享同一个PVC。 [cite: 1570]
        * [cite_start]**`Stateful Service` (有状态服务)[cite: 1591, 1595]:**
            * **适用场景:** 数据库、消息队列、分布式协调服务等。
            * **控制器:** `StatefulSet`。
            * **特点:** 每个Pod都有独一无二的、稳定的身份（“宠物”）。
            * [cite_start]**网络:** 必须配合`Headless Service`，为每个Pod提供稳定的、可单独访问的DNS记录。 [cite: 1671, 1675]
            * [cite_start]**存储:** 通过`volumeClaimTemplates`为每个Pod自动创建和绑定一个独立的PVC，实现数据的持久化和隔离。 [cite: 1655, 1657]
* #### **《实战》结合**：有状态服务
PV生命周期状态 → 《项目5》P5
StatefulSet稳定DNS → 《项目8》P16
有状态应用备份（Velero）→ 《项目5》P28
Headless Service原理 → 《项目8》P16  
MySQL主从Operator → 《项目8》P22

---

### **第十六周: 课堂讲授与实验 (理论1学时, 实验1学时)**
> 第七章 行为模式

* #### **本周议题 (理论): 如何让内外世界与我的K8s应用安全通信？**
    * **核心问题:** “我的微服务部署在K8s集群内部，它们之间如何相互发现和调用？同时，我如何将前端Web服务暴露给公网用户访问？”
    * **解决方案: 服务发现的两面性**
        * [cite_start]**内部服务发现:** 重温`Service`资源，它是K8s内部服务通信的基石。 [cite: 1772] 客户端通过稳定的Service DNS名称访问，K8s负责将请求转发到后端健康的Pod。
        * [cite_start]**外部服务发现:** 重温`Ingress`资源，它是从集群外部访问内部HTTP/S服务的标准和推荐方式。`Ingress`作为流量入口，可以根据域名、路径等规则，将请求智能路由到不同的内部`Service`。 [cite: 1887, 1888, 1900] `LoadBalancer`类型的Service是另一种方式，但成本更高。
    * #### **动手实践 (实验)**
        * **目标:** 搭建一个完整的内外通信链路。
        * 部署两个内部服务：`api-service`和`user-service`。
        * 让`api-service`通过`user-service`的`ClusterIP` Service名称来调用它。
        * 创建一个`Ingress`资源，配置当用户访问`app.yourdomain.com/api`时，流量被转发到`api-service`。
        * 在本地配置hosts文件或使用真实域名，通过浏览器或`curl`验证从外部可以成功访问到`api-service`。
* #### **《实战》结合**：DevOps实践
Jenkins Pipeline语法 → 《项目10》P10
Harbor镜像推送 → 《项目10》P8 
GitLab CI/CD集成 → 《项目10》P15
多环境发布策略 → 《项目10》P22
